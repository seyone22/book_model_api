# -*- coding: utf-8 -*-
"""Content-Based Filtering Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F6nvqkvB-KIodVZIimiC9Db9XS6gr01n

# Setup
"""

import pandas as pd
from ast import literal_eval

# Set the float format
pd.options.display.float_format = '{:.2f}'.format

# Import data from the goodbooks-10k repo
books_df = pd.read_csv('https://raw.githubusercontent.com/malcolmosh/goodbooks-10k/master/books_enriched.csv', index_col=[0], converters={"genres": literal_eval})
books_ratings = pd.read_csv('https://raw.githubusercontent.com/malcolmosh/goodbooks-10k/master/ratings.csv')

# Display three entries
books_df.head(3).T

"""## Analysis"""

# See what genres are numbers
non_list_entries = books_df[~books_df['genres'].apply(lambda x: isinstance(x, list))]
print(non_list_entries[['title', 'genres']])

print(books_df['title'].sample(10))



"""# Preprocessing

We're going to keep the following columns.


1.   authors
2.   average_rating
3.   genres
4.   language_code
5.   title
6.   description
"""

# Columns of the dataset we are interested in for this model
columns_to_keep = \
  ['authors', 'average_rating', 'genres', 'language_code', 'title', 'description']

# Subset of the dataset with only the above columns
books_df_subset = books_df[columns_to_keep]

# Extract the first author of the authors list and use it.
books_df_subset['author'] = books_df_subset['authors'].apply(lambda x: x[0]).astype(str)
# Count of unique authors
count_of_unique_authors = books_df_subset['author'].nunique()

# Remove NaNs from the description column
books_df_subset['description'] = books_df_subset['description'].fillna('')

# Remove NaNs from the original_title column
books_df_subset['title'] = books_df_subset['title'].fillna('')

"""# Feature Extraction"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import MultiLabelBinarizer

vectorizer = TfidfVectorizer()
hasher = FeatureHasher(n_features=count_of_unique_authors, input_type='string')
mlb = MultiLabelBinarizer()

# Hash the authors
#author_features = hasher.transform(books_df_subset['author'])

# Binarize the genres column
binarized_genres = mlb.fit_transform(books_df_subset['genres'])

# One-hot encode the language_code
books_df_subset = pd.get_dummies(books_df_subset, columns=['language_code'])

# Vectorize the title column
title_features  = vectorizer.fit_transform(books_df_subset['title'])

# Vectorize the description column
description_features = vectorizer.fit_transform(books_df_subset['description'])

from scipy.sparse import hstack

# Composite feature Vector
composite_feature_vector = hstack([binarized_genres, title_features, description_features])

"""# Similarity Measure

### Cosine Similarity
"""

from sklearn.metrics.pairwise import cosine_similarity
cosine_sim = cosine_similarity(composite_feature_vector)

"""### Euclidean Distance

### Manhattan Distance

### Jaccard Similarity

# Test
"""

indices = pd.Series(books_df_subset.index, index=books_df_subset['title']).drop_duplicates()

def recommend_items(title, cosine_sim=cosine_sim):
    # Get the index of the item that matches the title
    idx = indices[title]

    # Get the pairwsie similarity scores of all items with that item
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the items based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar items
    sim_scores = sim_scores[1:11]

    # Get the item indices
    item_indices = [i[0] for i in sim_scores]

    # Return the top 10 most similar items
    return books_df_subset['title'].iloc[item_indices]

print(recommend_items('Festive in Death (In Death, #39)'))

